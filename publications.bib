@INPROCEEDINGS{10008005,
  author={Lai, Zhiping and Kang, Xiaoyang and Wang, Hongbo and Zhang, Xueze and Zhang, Weiqi and Wang, Fuhao},
  booktitle={2022 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={Contrastive Domain Adaptation: A Self-Supervised Learning Framework for sEMG-Based Gesture Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  keywords={Human computer interaction;Electric potential;Biometrics (access control);Gesture recognition;Self-supervised learning;Electromyography;Calibration},
  doi={10.1109/IJCB54206.2022.10008005}}

  @InProceedings{10.1007/978-3-030-92238-2_3,
  author="Lai, Zhiping
  and Kang, Xiaoyang
  and Wang, Hongbo
  and Zhang, Weiqi
  and Zhang, Xueze
  and Gong, Peixian
  and Niu, Lan
  and Huang, Huijie",
  editor="Mantoro, Teddy
  and Lee, Minho
  and Ayu, Media Anugerah
  and Wong, Kok Wai
  and Hidayanto, Achmad Nizar",
  title="STCN-GR: Spatial-Temporal Convolutional Networks forÂ Surface-Electromyography-Based Gesture Recognition",
  booktitle="Neural Information Processing",
  year="2021",
  publisher="Springer International Publishing",
  address="Cham",
  pages="27--39",
  abstract="Gesture recognition using surface electromyography (sEMG) is the technical core of muscle-computer interface (MCI) in human-computer interaction (HCI), which aims to classify gestures according to signals obtained from human hands. Since sEMG signals are characterized by spatial relevancy and temporal nonstationarity, sEMG-based gesture recognition is a challenging task. Previous works attempt to model this structured information and extract spatial and temporal features, but the results are not satisfactory. To tackle this problem, we proposed spatial-temporal convolutional networks for sEMG-based gesture recognition (STCN-GR). In this paper, the concept of the sEMG graph is first proposed by us to represent sEMG data instead of image and vector sequence adopted by previous works, which provides a new perspective for the research of sEMG-based tasks, not just gesture recognition. Graph convolutional networks (GCNs) and temporal convolutional networks (TCNs) are used in STCN-GR to capture spatial-temporal information. Additionally, the connectivity of the graph can be adjusted adaptively in different layers of networks, which increases the flexibility of networks compared with the fixed graph structure used by original GCNs. On two high-density sEMG (HD-sEMG) datasets and a sparse armband dataset, STCN-GR outperforms previous works and achieves the state-of-the-art, which shows superior performance and powerful generalization ability.",
  isbn="978-3-030-92238-2"
  }
  
@article{Dong_2021,
doi = {10.1088/1742-6596/2003/1/012006},
url = {https://dx.doi.org/10.1088/1742-6596/2003/1/012006},
year = {2021},
month = {aug},
publisher = {IOP Publishing},
volume = {2003},
number = {1},
pages = {012006},
author = {Dong, Yu and Wang, Hongbo and Luo, Jingjing and Lai, Zhiping and Wang, Fuhao and Wang, Jiawei},
title = {Semantic Segmentation of Surgical Instruments based on Enhanced Multi-scale Receptive Field},
journal = {Journal of Physics: Conference Series},
abstract = {With the rapid development of robot assisted surgery, the segmentation of surgical instruments becomes more and more important. However, compared with the natural scene segmentation, surgical instrument segmentation is more difficult. To solve this problem, we improve a high and low resolution fusion module, which aims to extract detail information and context information from the fusion feature map of high and low resolution. Then, in the last layer of the encoder, we propose the Enhanced Multi-scale Receptive Field module to generate more available receptive fields. Our method is validated on 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset, and the result is better than the other methods. The extended experiment is carried out on the dataset of our surgical soft robot which has a content implementation.}
}

  